#!/usr/bin/env python3
"""
AI Honesty Enforcer - Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØµØ¯Ù‚ Ø§Ù„Ù…Ø·Ù„Ù‚
ÙŠÙ…Ù†Ø¹ Ø§Ù„Ù…Ø¨Ø§Ù„ØºØ© ÙˆÙŠØ¬Ø¨Ø± Ø¹Ù„Ù‰ Ø§Ù„ØµØ±Ø§Ø­Ø© Ø§Ù„ØªØ§Ù…Ø© ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
"""

import subprocess
import sys
import json
import os
import time
from datetime import datetime
from pathlib import Path
import tempfile


class AIHonestyEnforcer:
    """
    Ù…ÙÙ†ÙØ° Ø§Ù„ØµØ¯Ù‚ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    ÙŠØ¬Ø¨Ø± Ø¹Ù„Ù‰ Ø§Ù„ØµØ±Ø§Ø­Ø© Ø§Ù„ØªØ§Ù…Ø© ÙˆØ¹Ø¯Ù… Ø§Ù„Ù…Ø¨Ø§Ù„ØºØ©
    """
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.reports_dir = self.project_root / "ai_reports"
        self.reports_dir.mkdir(exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def enforce_honesty_check(self) -> dict:
        """
        ÙØ­Øµ Ø§Ù„ØµØ¯Ù‚ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ - Ù„Ø§ Ù…Ø¬Ø§Ù„ Ù„Ù„Ù…Ø¨Ø§Ù„ØºØ©
        """
        print("ğŸš¨ Ø¨Ø¯Ø¡ ÙØ­Øµ Ø§Ù„ØµØ¯Ù‚ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...")
        print("ğŸ“‹ Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©: Ù„Ø§ Ù…Ø¨Ø§Ù„ØºØ©ØŒ Ù„Ø§ ØªØ¬Ù…ÙŠÙ„ØŒ ØµØ±Ø§Ø­Ø© Ù…Ø·Ù„Ù‚Ø©")
        
        results = {
            "timestamp": self.timestamp,
            "honesty_enforced": True,
            "tests": {},
            "screenshots": {},
            "failures": {},
            "truth_score": 0,
            "ai_compliance": False
        }
        
        # 1. ÙØ­Øµ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
        test_results = self._run_real_tests()
        results["tests"] = test_results
        
        # 2. ÙØ­Øµ Ø§Ù„ØªØºØ·ÙŠØ© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
        coverage_results = self._run_real_coverage()
        results["coverage"] = coverage_results
        
        # 3. ÙØ­Øµ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
        quality_results = self._run_real_quality_check()
        results["quality"] = quality_results
        
        # 4. Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØµØ¯Ù‚
        results["truth_score"] = self._calculate_truth_score(results)
        
        # 5. ØªÙ‚ÙŠÙŠÙ… Ø§Ù…ØªØ«Ø§Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        results["ai_compliance"] = self._evaluate_ai_compliance(results)
        
        # 6. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        self._generate_honesty_report(results)
        
        return results
    
    def _run_real_tests(self) -> dict:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ¬Ù…ÙŠÙ„"""
        print("ğŸ§ª ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©...")
        
        try:
            # ØªØ´ØºÙŠÙ„ pytest Ù…Ø¹ ØªÙØ§ØµÙŠÙ„ ÙƒØ§Ù…Ù„Ø©
            result = subprocess.run([
                sys.executable, "-m", "pytest",
                "--tb=long",
                "--verbose",
                "--no-header",
                "--json-report",
                f"--json-report-file={self.reports_dir}/test_results.json"
            ], 
            capture_output=True, 
            text=True, 
            timeout=300,
            env={**os.environ, "DJANGO_SETTINGS_MODULE": "config.settings"}
            )
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
            test_data = self._read_test_json()
            
            passed = test_data.get("summary", {}).get("passed", 0)
            failed = test_data.get("summary", {}).get("failed", 0)
            errors = test_data.get("summary", {}).get("error", 0)
            total = passed + failed + errors
            
            # Ø­ÙØ¸ screenshot Ø¥Ø°Ø§ Ù†Ø¬Ø­Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
            screenshot_path = None
            if failed == 0 and errors == 0 and total > 0:
                screenshot_path = self._take_success_screenshot(test_data)
            
            return {
                "status": "passed" if failed == 0 and errors == 0 else "failed",
                "total": total,
                "passed": passed,
                "failed": failed,
                "errors": errors,
                "success_rate": (passed / total * 100) if total > 0 else 0,
                "screenshot": screenshot_path,
                "raw_output": result.stdout,
                "raw_errors": result.stderr,
                "honest_assessment": self._get_honest_test_assessment(passed, failed, errors, total)
            }
            
        except subprocess.TimeoutExpired:
            return {
                "status": "timeout",
                "honest_assessment": "Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØªØ¬Ø§ÙˆØ²Øª Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯ - Ù…Ø´ÙƒÙ„Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© ØªØ­ØªØ§Ø¬ Ø­Ù„"
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "honest_assessment": f"Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: {str(e)}"
            }
    
    def _run_real_coverage(self) -> dict:
        """ÙØ­Øµ Ø§Ù„ØªØºØ·ÙŠØ© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ¬Ù…ÙŠÙ„"""
        print("ğŸ“Š ÙØ­Øµ Ø§Ù„ØªØºØ·ÙŠØ© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©...")
        
        try:
            result = subprocess.run([
                sys.executable, "-m", "pytest",
                "--cov=.",
                "--cov-report=json",
                "--cov-report=term-missing",
                "--quiet"
            ], 
            capture_output=True, 
            text=True,
            env={**os.environ, "DJANGO_SETTINGS_MODULE": "config.settings"}
            )
            
            # Ù‚Ø±Ø§Ø¡Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØºØ·ÙŠØ©
            coverage_data = self._read_coverage_json()
            
            if coverage_data:
                total_coverage = coverage_data.get("totals", {}).get("percent_covered", 0)
                
                return {
                    "percentage": round(total_coverage, 2),
                    "meets_requirement": total_coverage >= 90,
                    "honest_assessment": self._get_honest_coverage_assessment(total_coverage),
                    "details": coverage_data.get("totals", {}),
                    "files_below_threshold": self._get_low_coverage_files(coverage_data)
                }
            else:
                return {
                    "percentage": 0,
                    "meets_requirement": False,
                    "honest_assessment": "ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØºØ·ÙŠØ© - Ù…Ø´ÙƒÙ„Ø© ØªÙ‚Ù†ÙŠØ© Ø­Ù‚ÙŠÙ‚ÙŠØ©"
                }
                
        except Exception as e:
            return {
                "percentage": 0,
                "meets_requirement": False,
                "error": str(e),
                "honest_assessment": f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„ØªØºØ·ÙŠØ©: {str(e)}"
            }
    
    def _run_real_quality_check(self) -> dict:
        """ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ"""
        print("ğŸ” ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ...")
        
        quality_results = {}
        
        # ÙØ­Øµ black
        try:
            black_result = subprocess.run([
                "black", "--check", "--diff", "."
            ], capture_output=True, text=True)
            
            quality_results["formatting"] = {
                "tool": "black",
                "passed": black_result.returncode == 0,
                "issues": black_result.stdout if black_result.returncode != 0 else None
            }
        except:
            quality_results["formatting"] = {
                "tool": "black",
                "passed": False,
                "error": "black ØºÙŠØ± Ù…ØªØ§Ø­"
            }
        
        # ÙØ­Øµ flake8
        try:
            flake8_result = subprocess.run([
                "flake8", "."
            ], capture_output=True, text=True)
            
            quality_results["linting"] = {
                "tool": "flake8",
                "passed": flake8_result.returncode == 0,
                "issues": flake8_result.stdout if flake8_result.returncode != 0 else None
            }
        except:
            quality_results["linting"] = {
                "tool": "flake8", 
                "passed": False,
                "error": "flake8 ØºÙŠØ± Ù…ØªØ§Ø­"
            }
        
        # ØªÙ‚ÙŠÙŠÙ… ØµØ§Ø¯Ù‚ Ù„Ù„Ø¬ÙˆØ¯Ø©
        all_passed = all(
            check.get("passed", False) 
            for check in quality_results.values()
        )
        
        quality_results["overall"] = {
            "passed": all_passed,
            "honest_assessment": self._get_honest_quality_assessment(quality_results)
        }
        
        return quality_results
    
    def _take_success_screenshot(self, test_data: dict) -> str:
        """Ø£Ø®Ø° screenshot Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø© ÙÙ‚Ø·"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± HTML Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©
            html_content = self._generate_success_html(test_data)
            
            # Ø­ÙØ¸ HTML Ù…Ø¤Ù‚Øª
            with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as f:
                f.write(html_content)
                html_path = f.name
            
            # Ø£Ø®Ø° screenshot (Ù…Ø­Ø§ÙƒØ§Ø© - ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù†Ø³ØªØ®Ø¯Ù… selenium)
            screenshot_path = self.reports_dir / f"success_tests_{self.timestamp}.png"
            
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø£Ø®Ø° screenshot
            self._simulate_screenshot(html_path, screenshot_path)
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
            os.unlink(html_path)
            
            return str(screenshot_path)
            
        except Exception as e:
            print(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø£Ø®Ø° screenshot: {e}")
            return None
    
    def _simulate_screenshot(self, html_path: str, output_path: Path):
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø£Ø®Ø° screenshot"""
        # ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©ØŒ Ù†Ø³ØªØ®Ø¯Ù…:
        # from selenium import webdriver
        # driver = webdriver.Chrome()
        # driver.get(f"file://{html_path}")
        # driver.save_screenshot(str(output_path))
        
        # Ù„Ù„Ù…Ø­Ø§ÙƒØ§Ø©ØŒ Ù†Ù†Ø´Ø¦ Ù…Ù„Ù Ù†ØµÙŠ
        with open(output_path.with_suffix('.txt'), 'w') as f:
            f.write(f"Screenshot placeholder for successful tests at {self.timestamp}")
    
    def _generate_success_html(self, test_data: dict) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ HTML Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©"""
        passed = test_data.get("summary", {}).get("passed", 0)
        total = test_data.get("summary", {}).get("total", 0)
        
        return f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©</title>
            <style>
                body {{ font-family: Arial, sans-serif; background: #f0f8ff; }}
                .success {{ color: #28a745; font-size: 24px; font-weight: bold; }}
                .stats {{ background: #d4edda; padding: 20px; border-radius: 10px; }}
            </style>
        </head>
        <body>
            <h1 class="success">âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø¬Ø­Øª!</h1>
            <div class="stats">
                <p>Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: {total}</p>
                <p>Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {passed}</p>
                <p>Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: 100%</p>
                <p>Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
        </body>
        </html>
        """
    
    def _read_test_json(self) -> dict:
        """Ù‚Ø±Ø§Ø¡Ø© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ù† JSON"""
        json_path = self.reports_dir / "test_results.json"
        if json_path.exists():
            try:
                with open(json_path) as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def _read_coverage_json(self) -> dict:
        """Ù‚Ø±Ø§Ø¡Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØºØ·ÙŠØ© Ù…Ù† JSON"""
        coverage_path = self.project_root / "coverage.json"
        if coverage_path.exists():
            try:
                with open(coverage_path) as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def _get_honest_test_assessment(self, passed: int, failed: int, errors: int, total: int) -> str:
        """ØªÙ‚ÙŠÙŠÙ… ØµØ§Ø¯Ù‚ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
        if total == 0:
            return "âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª - Ù‡Ø°Ø§ ØºÙŠØ± Ù…Ù‚Ø¨ÙˆÙ„ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹"
        
        if failed > 0 or errors > 0:
            return f"âŒ ÙØ´Ù„ {failed + errors} Ù…Ù† {total} Ø§Ø®ØªØ¨Ø§Ø± - ÙŠØ¬Ø¨ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ù‚Ø¨Ù„ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©"
        
        if passed < 5:
            return f"âš ï¸ Ù†Ø¬Ø­ {passed} Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙÙ‚Ø· - Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„ Ø¬Ø¯Ø§Ù‹ØŒ ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"
        
        return f"âœ… Ù†Ø¬Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ({passed}/{total}) - ÙˆØ¶Ø¹ Ø¬ÙŠØ¯"
    
    def _get_honest_coverage_assessment(self, coverage: float) -> str:
        """ØªÙ‚ÙŠÙŠÙ… ØµØ§Ø¯Ù‚ Ù„Ù„ØªØºØ·ÙŠØ©"""
        if coverage < 50:
            return f"âŒ ØªØºØ·ÙŠØ© Ø¶Ø¹ÙŠÙØ© Ø¬Ø¯Ø§Ù‹ ({coverage:.1f}%) - ØºÙŠØ± Ù…Ù‚Ø¨ÙˆÙ„Ø© Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹"
        elif coverage < 70:
            return f"âš ï¸ ØªØºØ·ÙŠØ© Ù…Ù†Ø®ÙØ¶Ø© ({coverage:.1f}%) - ØªØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† ÙƒØ¨ÙŠØ±"
        elif coverage < 90:
            return f"ğŸ”¶ ØªØºØ·ÙŠØ© Ù…ØªÙˆØ³Ø·Ø© ({coverage:.1f}%) - Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (90%)"
        else:
            return f"âœ… ØªØºØ·ÙŠØ© Ù…Ù…ØªØ§Ø²Ø© ({coverage:.1f}%) - ØªØªØ¬Ø§ÙˆØ² Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"
    
    def _get_honest_quality_assessment(self, quality_results: dict) -> str:
        """ØªÙ‚ÙŠÙŠÙ… ØµØ§Ø¯Ù‚ Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯"""
        issues = []
        
        for check_name, check_result in quality_results.items():
            if check_name == "overall":
                continue
                
            if not check_result.get("passed", False):
                if "error" in check_result:
                    issues.append(f"{check_result['tool']} ØºÙŠØ± Ù…ØªØ§Ø­")
                else:
                    issues.append(f"{check_result['tool']} ÙˆØ¬Ø¯ Ù…Ø´Ø§ÙƒÙ„")
        
        if issues:
            return f"âŒ Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø©: {', '.join(issues)}"
        else:
            return "âœ… Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ Ù…Ù‚Ø¨ÙˆÙ„Ø©"
    
    def _get_low_coverage_files(self, coverage_data: dict) -> list:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø°Ø§Øª Ø§Ù„ØªØºØ·ÙŠØ© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©"""
        low_files = []
        files = coverage_data.get("files", {})
        
        for file_path, file_data in files.items():
            coverage = file_data.get("summary", {}).get("percent_covered", 0)
            if coverage < 90:
                low_files.append({
                    "file": file_path,
                    "coverage": round(coverage, 2)
                })
        
        return sorted(low_files, key=lambda x: x["coverage"])
    
    def _calculate_truth_score(self, results: dict) -> int:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØµØ¯Ù‚ (0-100)"""
        score = 0
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        test_status = results.get("tests", {}).get("status")
        if test_status == "passed":
            score += 40
        elif test_status == "failed":
            score += 10  # Ù†Ù‚Ø§Ø· Ù„Ù„ØµØ¯Ù‚ ÙÙŠ Ø§Ù„Ø¥Ø¨Ù„Ø§Øº Ø¹Ù† Ø§Ù„ÙØ´Ù„
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„ØªØºØ·ÙŠØ©
        coverage = results.get("coverage", {}).get("percentage", 0)
        if coverage >= 90:
            score += 30
        elif coverage >= 70:
            score += 20
        elif coverage >= 50:
            score += 10
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©
        quality_passed = results.get("quality", {}).get("overall", {}).get("passed", False)
        if quality_passed:
            score += 30
        else:
            score += 10  # Ù†Ù‚Ø§Ø· Ù„Ù„ØµØ¯Ù‚ ÙÙŠ Ø§Ù„Ø¥Ø¨Ù„Ø§Øº Ø¹Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        
        return min(score, 100)
    
    def _evaluate_ai_compliance(self, results: dict) -> bool:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù…ØªØ«Ø§Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„ØµØ¯Ù‚"""
        # Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ù…ØªØ«Ù„ Ø¥Ø°Ø§:
        # 1. Ø£Ø¨Ù„Øº Ø¹Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
        # 2. Ù„Ù… ÙŠØ¨Ø§Ù„Øº ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        # 3. ÙƒØ§Ù† ØµØ§Ø¯Ù‚Ø§Ù‹ ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª
        
        truth_score = results.get("truth_score", 0)
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†Ù‚Ø§Ø· Ø¹Ø§Ù„ÙŠØ©ØŒ ÙØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØµØ§Ø¯Ù‚
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ù†Ø®ÙØ¶Ø© ÙˆÙ„ÙƒÙ† Ø£Ø¨Ù„Øº Ø¹Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ØŒ ÙÙ‡Ùˆ ØµØ§Ø¯Ù‚ Ø£ÙŠØ¶Ø§Ù‹
        
        return truth_score >= 50  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„ØµØ¯Ù‚
    
    def _generate_honesty_report(self, results: dict):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØµØ¯Ù‚ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        report_path = self.reports_dir / f"honesty_report_{self.timestamp}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ Ø£ÙŠØ¶Ø§Ù‹
        text_report_path = self.reports_dir / f"honesty_report_{self.timestamp}.txt"
        
        with open(text_report_path, 'w', encoding='utf-8') as f:
            f.write("ğŸš¨ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØµØ¯Ù‚ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"Ø§Ù„ØªØ§Ø±ÙŠØ®: {results['timestamp']}\n")
            f.write(f"Ù†Ù‚Ø§Ø· Ø§Ù„ØµØ¯Ù‚: {results['truth_score']}/100\n")
            f.write(f"Ø§Ù…ØªØ«Ø§Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {'âœ… Ù…Ù…ØªØ«Ù„' if results['ai_compliance'] else 'âŒ ØºÙŠØ± Ù…Ù…ØªØ«Ù„'}\n\n")
            
            # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
            tests = results.get("tests", {})
            f.write("ğŸ“‹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª:\n")
            f.write(f"  Ø§Ù„Ø­Ø§Ù„Ø©: {tests.get('status', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}\n")
            f.write(f"  Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØµØ§Ø¯Ù‚: {tests.get('honest_assessment', 'ØºÙŠØ± Ù…ØªØ§Ø­')}\n\n")
            
            # ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªØºØ·ÙŠØ©
            coverage = results.get("coverage", {})
            f.write("ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØºØ·ÙŠØ©:\n")
            f.write(f"  Ø§Ù„Ù†Ø³Ø¨Ø©: {coverage.get('percentage', 0):.1f}%\n")
            f.write(f"  Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØµØ§Ø¯Ù‚: {coverage.get('honest_assessment', 'ØºÙŠØ± Ù…ØªØ§Ø­')}\n\n")
            
            # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¬ÙˆØ¯Ø©
            quality = results.get("quality", {})
            f.write("ğŸ” Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¬ÙˆØ¯Ø©:\n")
            f.write(f"  Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØµØ§Ø¯Ù‚: {quality.get('overall', {}).get('honest_assessment', 'ØºÙŠØ± Ù…ØªØ§Ø­')}\n")


def main():
    """Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ÙØ­Øµ Ø§Ù„ØµØ¯Ù‚"""
    enforcer = AIHonestyEnforcer()
    
    print("ğŸš¨ Ø¨Ø¯Ø¡ ÙØ­Øµ Ø§Ù„ØµØ¯Ù‚ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ...")
    print("ğŸ“‹ Ù‡Ø°Ø§ Ø§Ù„ÙØ­Øµ ÙŠØ¬Ø¨Ø± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØµØ±Ø§Ø­Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©")
    print("ğŸš« Ù„Ø§ Ù…Ø¬Ø§Ù„ Ù„Ù„Ù…Ø¨Ø§Ù„ØºØ© Ø£Ùˆ Ø§Ù„ØªØ¬Ù…ÙŠÙ„")
    print()
    
    results = enforcer.enforce_honesty_check()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Ø¨ØµØ±Ø§Ø­Ø© Ù…Ø·Ù„Ù‚Ø©):")
    print("=" * 60)
    
    print(f"ğŸ¯ Ù†Ù‚Ø§Ø· Ø§Ù„ØµØ¯Ù‚: {results['truth_score']}/100")
    print(f"ğŸ¤– Ø§Ù…ØªØ«Ø§Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {'âœ… Ù…Ù…ØªØ«Ù„' if results['ai_compliance'] else 'âŒ ØºÙŠØ± Ù…Ù…ØªØ«Ù„'}")
    
    # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„ØµØ§Ø¯Ù‚Ø©
    tests = results.get("tests", {})
    if "honest_assessment" in tests:
        print(f"ğŸ§ª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: {tests['honest_assessment']}")
    
    coverage = results.get("coverage", {})
    if "honest_assessment" in coverage:
        print(f"ğŸ“Š Ø§Ù„ØªØºØ·ÙŠØ©: {coverage['honest_assessment']}")
    
    quality = results.get("quality", {})
    if "overall" in quality and "honest_assessment" in quality["overall"]:
        print(f"ğŸ” Ø§Ù„Ø¬ÙˆØ¯Ø©: {quality['overall']['honest_assessment']}")
    
    # Ø¹Ø±Ø¶ screenshot Ø¥Ø°Ø§ ÙˆØ¬Ø¯
    screenshot = tests.get("screenshot")
    if screenshot:
        print(f"ğŸ“¸ Screenshot Ù„Ù„Ù†Ø¬Ø§Ø­: {screenshot}")
    
    print("\nğŸš¨ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙŠØ¶Ù…Ù† Ø§Ù„ØµØ¯Ù‚ Ø§Ù„Ù…Ø·Ù„Ù‚ - Ù„Ø§ Ù…Ø¬Ø§Ù„ Ù„Ù„Ù…Ø¨Ø§Ù„ØºØ©!")
    
    # Ø¥Ø±Ø¬Ø§Ø¹ ÙƒÙˆØ¯ Ø§Ù„Ø®Ø±ÙˆØ¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
    if results["ai_compliance"] and results["truth_score"] >= 70:
        return 0
    else:
        return 1


if __name__ == "__main__":
    sys.exit(main())
